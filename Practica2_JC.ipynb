{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f055968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 1) Cargue la base de datos Machines.csv a un DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Machines.csv\")  # si estás en Colab, sube el archivo y ajusta la ruta\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2115f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2) Proporción de cada clase de Failure y balanceo\n",
    "\n",
    "proporciones = df[\"Failure\"].value_counts(normalize=True).sort_index()\n",
    "conteos = df[\"Failure\"].value_counts().sort_index()\n",
    "\n",
    "display(pd.DataFrame({\"conteo\": conteos, \"proporción\": proporciones}))\n",
    "\n",
    "p_min = proporciones.min()\n",
    "if p_min < 0.4:\n",
    "    print(\"Conclusión: el conjunto está DESBALANCEADO (al menos una clase < 40%).\")\n",
    "else:\n",
    "    print(\"Conclusión: el conjunto está BALANCEADO (todas las clases ≥ 40%).\")\n",
    "\n",
    "print(\"Nota: aunque no sea extremo, 68/32 suele considerarse desbalance moderado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 3) Defina X (predictoras) y y (objetivo)\n",
    "\n",
    "X = df.drop(columns=[\"Failure\"])\n",
    "y = df[\"Failure\"]\n",
    "\n",
    "print(\"Predictoras (X):\", list(X.columns))\n",
    "print(\"Objetivo (y): Failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a21dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 4) Train/Test split 80/20 con estratificación (random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Shapes -> X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"Proporciones en train:\", y_train.value_counts(normalize=True).to_dict())\n",
    "print(\"Proporciones en test :\", y_test.value_counts(normalize=True).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21539ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 5) Estandarice variables predictoras\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)  # se ajusta SOLO con train\n",
    "X_test_sc  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Media (train, aprox 0):\", X_train_sc.mean(axis=0).round(3))\n",
    "print(\"Std  (train, aprox 1):\", X_train_sc.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 6) Genere modelos: SVM y Árbol de Decisión\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "svm_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")  # baseline razonable\n",
    "tree_model = DecisionTreeClassifier(random_state=42)  # baseline\n",
    "\n",
    "svm_model.fit(X_train_sc, y_train)\n",
    "tree_model.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Modelos entrenados: SVM y Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d919a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 7) Métricas + matriz de confusión para cada modelo\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def evaluar_modelo(nombre, model, Xte, yte, usa_decision_function=False):\n",
    "    y_pred = model.predict(Xte)\n",
    "    cm = confusion_matrix(yte, y_pred)  # orden: [[TN, FP],[FN, TP]] si clases 0/1\n",
    "    out = {\n",
    "        \"modelo\": nombre,\n",
    "        \"accuracy\": accuracy_score(yte, y_pred),\n",
    "        \"precision\": precision_score(yte, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(yte, y_pred),\n",
    "        \"f1\": f1_score(yte, y_pred),\n",
    "        \"TN\": cm[0,0], \"FP\": cm[0,1], \"FN\": cm[1,0], \"TP\": cm[1,1],\n",
    "    }\n",
    "    # ROC AUC (opcional pero útil)\n",
    "    if usa_decision_function:\n",
    "        scores = model.decision_function(Xte)\n",
    "        out[\"roc_auc\"] = roc_auc_score(yte, scores)\n",
    "    else:\n",
    "        proba = model.predict_proba(Xte)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "        out[\"roc_auc\"] = roc_auc_score(yte, proba) if proba is not None else np.nan\n",
    "\n",
    "    print(f\"\\n=== {nombre} ===\")\n",
    "    print(\"Matriz de confusión [ [TN FP], [FN TP] ]:\\n\", cm)\n",
    "    print(\"\\nReporte de clasificación:\\n\", classification_report(yte, y_pred, digits=4))\n",
    "    return out\n",
    "\n",
    "resultados = []\n",
    "resultados.append(evaluar_modelo(\"SVM (RBF)\", svm_model, X_test_sc, y_test, usa_decision_function=True))\n",
    "resultados.append(evaluar_modelo(\"Decision Tree\", tree_model, X_test_sc, y_test, usa_decision_function=False))\n",
    "\n",
    "res_df = pd.DataFrame(resultados).set_index(\"modelo\")\n",
    "display(res_df[[\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"TN\",\"FP\",\"FN\",\"TP\"]].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0563d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 8) ¿Qué modelo es mejor y por qué? (basado en métricas)\n",
    "\n",
    "# Regla práctica: en mantenimiento suele pesar fuerte Recall/F1 del 'Failure=1'.\n",
    "# Aquí hacemos una comparación simple por F1 y ROC-AUC.\n",
    "mejor_por_f1 = res_df[\"f1\"].idxmax()\n",
    "mejor_por_auc = res_df[\"roc_auc\"].idxmax()\n",
    "\n",
    "print(\"Mejor por F1 :\", mejor_por_f1, \"-> F1 =\", float(res_df.loc[mejor_por_f1, \"f1\"]))\n",
    "print(\"Mejor por AUC:\", mejor_por_auc, \"-> AUC =\", float(res_df.loc[mejor_por_auc, \"roc_auc\"]))\n",
    "\n",
    "if mejor_por_f1 == mejor_por_auc:\n",
    "    print(f\"Conclusión: {mejor_por_f1} es el mejor modelo global (mejor F1 y mejor AUC).\")\n",
    "else:\n",
    "    print(\"Conclusión: depende de la métrica priorizada. Si priorizas F1 el mejor es:\", mejor_por_f1,\n",
    "          \"y si priorizas AUC el mejor es:\", mejor_por_auc)\n",
    "\n",
    "# Comentario típico: SVM suele generalizar mejor que un árbol sin poda/tuning en datos continuos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eacfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 9) Mantenimiento: ¿Qué métrica es más importante?\n",
    "\n",
    "# Suposición estándar: 'Failure=1' significa falla (evento raro y costoso).\n",
    "# En mantenimiento predictivo, perder una falla (FN) suele ser más caro que una alarma extra (FP).\n",
    "# Por eso, la métrica clave suele ser RECALL (sensibilidad) de la clase Failure=1.\n",
    "\n",
    "for modelo in res_df.index:\n",
    "    recall = float(res_df.loc[modelo, \"recall\"])\n",
    "    fn = int(res_df.loc[modelo, \"FN\"])\n",
    "    print(f\"{modelo}: Recall(Failure=1) = {recall:.4f} | FN = {fn}\")\n",
    "\n",
    "print(\"\\nRespuesta: la métrica más importante suele ser el RECALL de Failure=1 (minimiza falsos negativos).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 10) Mantenimiento: ¿qué error es más grave, FP o FN?\n",
    "\n",
    "# Tomamos el mejor modelo por F1 para ilustrar.\n",
    "best = mejor_por_f1\n",
    "fp = int(res_df.loc[best, \"FP\"])\n",
    "fn = int(res_df.loc[best, \"FN\"])\n",
    "\n",
    "print(\"Modelo ilustrativo:\", best)\n",
    "print(\"FP (falsos positivos):\", fp, \"-> alarma de falla cuando no falla\")\n",
    "print(\"FN (falsos negativos):\", fn, \"-> NO detectas una falla real\")\n",
    "\n",
    "print(\"\"\"\\nRespuesta: en mantenimiento suele ser más grave el FALSO NEGATIVO (FN).\n",
    "Justificación: predices 'no falla' pero sí falla -> paros no planificados, daño a equipos, riesgos de seguridad y costos altos.\n",
    "Un FP normalmente solo implica una inspección/mantenimiento extra (costo menor y controlable).\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
